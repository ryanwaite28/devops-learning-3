‚úÖ Next: Phase 4A ‚Äî Secure Supply Chain in Jenkins (Enterprise-grade)
What you‚Äôll implement

A single pipeline that produces:

SBOM (Syft)

SAST (Semgrep)

Dependency + container scan (Trivy)

Policy gate (fail on critical/high, configurable)

Attestation artifacts (saved as build artifacts; optional Cosign signing later)

We‚Äôll keep the ‚Äúapp‚Äù simple; the platform pipeline is the product.

Phase 4A.1 ‚Äî Current Task
Build a hardened ‚Äútools‚Äù image for CI

Right now your tools container is alpine:3.20. That‚Äôs great for proving multi-container, but enterprise CI needs pinned tooling.

Background (why this matters)

Pulling tools at runtime is slow + flaky

Supply chain security demands pinned versions

Reproducibility demands immutable CI environments

Deliverable

Create a Dockerfile in your repo:

pipelines/agent-images/ci-tools/Dockerfile


Build an image that includes:

bash, curl, git, jq, yq

trivy

syft

semgrep

Recommended base

Use a stable base with good package support:

alpine:3.20 (fine) or

debian:bookworm-slim (more compatible; slightly heavier)

I recommend debian-slim for fewer tool install headaches.

Example Dockerfile (use this)
FROM debian:bookworm-slim

ARG TARGETARCH
SHELL ["/bin/bash", "-euxo", "pipefail", "-c"]

RUN apt-get update && apt-get install -y --no-install-recommends \
    ca-certificates curl git jq bash coreutils \
  && rm -rf /var/lib/apt/lists/*

# yq (mikefarah) pinned
ARG YQ_VERSION=v4.44.3
RUN curl -fsSL -o /usr/local/bin/yq \
    "https://github.com/mikefarah/yq/releases/download/${YQ_VERSION}/yq_linux_${TARGETARCH}" \
 && chmod +x /usr/local/bin/yq

# Trivy pinned
ARG TRIVY_VERSION=0.56.2
RUN curl -fsSL "https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_Linux-${TARGETARCH}.tar.gz" \
  | tar -xz -C /usr/local/bin trivy

# Syft pinned
ARG SYFT_VERSION=1.16.0
RUN curl -fsSL "https://github.com/anchore/syft/releases/download/v${SYFT_VERSION}/syft_${SYFT_VERSION}_linux_${TARGETARCH}.tar.gz" \
  | tar -xz -C /usr/local/bin syft

# Semgrep pinned (python)
RUN apt-get update && apt-get install -y --no-install-recommends python3 python3-pip \
 && pip3 install --no-cache-dir semgrep==1.103.0 \
 && rm -rf /var/lib/apt/lists/*

RUN useradd -m -u 10001 ci && mkdir -p /workspace && chown -R ci:ci /workspace
USER ci
WORKDIR /workspace

CMD ["bash", "-lc", "sleep infinity"]

Build it locally

From repo root:

docker build -t ci-tools:0.1.0 pipelines/agent-images/ci-tools

Load it into kind (so Kubernetes can pull it without Docker Hub)
kind load docker-image ci-tools:0.1.0 --name kube-lab


‚úÖ This eliminates ErrImagePull and rate limits.

Acceptance criteria (Definition of Done)

Run these:

docker run --rm ci-tools:0.1.0 trivy --version
docker run --rm ci-tools:0.1.0 syft version
docker run --rm ci-tools:0.1.0 semgrep --version
docker run --rm ci-tools:0.1.0 yq --version


And:

kind load docker-image ci-tools:0.1.0 --name kube-lab


Commit:

feat: add pinned ci-tools image for secure pipeline stages

What comes immediately after (Phase 4A.2)

Update your Jenkins pod YAML to use:

tools container image: ci-tools:0.1.0 (local)
Then add pipeline stages:

Checkout

SAST (Semgrep)

SBOM (Syft)

Trivy FS scan

Gate (fail on findings)

Archive artifacts

Your move

Do Phase 4A.1 now.

When you‚Äôre done, reply with:

the output of the four --version checks

confirmation you ran kind load docker-image ...

Then I‚Äôll give you the exact Jenkinsfile for Phase 4A.2 (with gates + artifacts) and we‚Äôll keep marching toward C (Argo CD GitOps) and B (runtime governance).


-------

Is it possible to add in using SonarQube?

Yes ‚Äî and SonarQube fits perfectly into Option A (secure supply chain CI). In many enterprises it‚Äôs the ‚Äúquality gate‚Äù that blocks merges/releases.

There are two clean ways to add it to your lab:

Run SonarQube as a platform service in Docker Compose (recommended for your current architecture)

Later, move it into Kubernetes (optional ‚Äúplatform maturity‚Äù milestone)

Below is the enterprise-style approach that keeps everything reproducible and GitHub-ready.

‚úÖ Recommended: Add SonarQube to Docker Compose (Phase 4A)
Why this is a good fit

Jenkins controller is already in Docker Compose

SonarQube is stateful and heavier; Compose is a great place for it in a local lab

Keeps Kubernetes focused on ephemeral execution (agents), not platform databases

1) Add SonarQube + Postgres to docker-compose.yml

Add these services:

sonarqube-db:
  image: postgres:16.2
  restart: unless-stopped
  environment:
    POSTGRES_USER: sonar
    POSTGRES_PASSWORD: ${SONAR_DB_PASSWORD}
    POSTGRES_DB: sonarqube
  volumes:
    - sonarqube-db-data:/var/lib/postgresql/data
  networks:
    - kube-platform-engineering-lab

sonarqube:
  image: sonarqube:community
  restart: unless-stopped
  depends_on:
    - sonarqube-db
  environment:
    SONAR_JDBC_URL: jdbc:postgresql://sonarqube-db:5432/sonarqube
    SONAR_JDBC_USERNAME: sonar
    SONAR_JDBC_PASSWORD: ${SONAR_DB_PASSWORD}
  ports:
    - "9000:9000"
  volumes:
    - sonarqube-data:/opt/sonarqube/data
    - sonarqube-extensions:/opt/sonarqube/extensions
    - sonarqube-logs:/opt/sonarqube/logs
  networks:
    - kube-platform-engineering-lab


Add volumes:

volumes:
  sonarqube-db-data:
  sonarqube-data:
  sonarqube-extensions:
  sonarqube-logs:


Add to your .env (and keep it gitignored):

SONAR_DB_PASSWORD=change-me

Important: one host setting

SonarQube needs a higher vm.max_map_count.

On macOS with Docker Desktop, set it in the Docker VM:

docker run --rm --privileged --pid=host alpine sh -c "sysctl -w vm.max_map_count=262144"


(If it resets after restart, we can add a note to README.)

Bring it up:

docker compose up -d sonarqube sonarqube-db


Visit:

http://localhost:9000
Default login (first-time):

admin / admin (it will force a change)

2) Wire Jenkins to SonarQube
A) Create a token in SonarQube

In SonarQube UI:

My Account ‚Üí Security ‚Üí Generate Token
Save it.

B) Store token in Jenkins Credentials

Jenkins ‚Üí Manage Jenkins ‚Üí Credentials ‚Üí (Global) ‚Üí Add:

Kind: Secret text

ID: sonar-token

Secret: paste token

C) Configure SonarQube server in Jenkins

Jenkins ‚Üí Manage Jenkins ‚Üí System ‚Üí SonarQube servers

Name: sonarqube

Server URL: http://sonarqube:9000 ‚úÖ (from Jenkins container, use Compose DNS)

Authentication token: select sonar-token

3) Add SonarQube Scanner to your CI ‚Äútools‚Äù image

Since you‚Äôre building a pinned ci-tools image anyway, add sonar-scanner there.

In your ci-tools Dockerfile (Debian example), add:

ARG SONAR_SCANNER_VERSION=6.2.1.4610
RUN curl -fsSL -o /tmp/sonar-scanner.zip \
    "https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-${SONAR_SCANNER_VERSION}-linux-x64.zip" \
 && apt-get update && apt-get install -y --no-install-recommends unzip \
 && unzip /tmp/sonar-scanner.zip -d /opt \
 && ln -s /opt/sonar-scanner-*/bin/sonar-scanner /usr/local/bin/sonar-scanner \
 && rm -f /tmp/sonar-scanner.zip \
 && rm -rf /var/lib/apt/lists/*


(You can pin versions like everything else.)

4) Pipeline stage: run analysis + enforce Quality Gate
Add a sonar-project.properties in your repo root

Example:

sonar.projectKey=kube-platform-engineering-lab
sonar.projectName=kube-platform-engineering-lab
sonar.sources=.
sonar.exclusions=**/node_modules/**,**/vendor/**,**/dist/**,**/build/**,**/.git/**

Jenkinsfile stage (Declarative)
stage('SonarQube Analysis') {
  steps {
    container('tools') {
      withSonarQubeEnv('sonarqube') {
        sh '''
          sonar-scanner \
            -Dsonar.projectKey=kube-platform-engineering-lab \
            -Dsonar.host.url=$SONAR_HOST_URL \
            -Dsonar.login=$SONAR_AUTH_TOKEN
        '''
      }
    }
  }
}

stage('Quality Gate') {
  steps {
    timeout(time: 5, unit: 'MINUTES') {
      waitForQualityGate abortPipeline: true
    }
  }
}

One extra step (required): SonarQube webhook

So waitForQualityGate can receive results, add a webhook in SonarQube:

SonarQube ‚Üí Administration ‚Üí Configuration ‚Üí Webhooks ‚Üí Create

URL: http://jenkins:8080/sonarqube-webhook/ ‚úÖ (SonarQube container can reach Jenkins via Compose DNS)

Now the pipeline can block based on Quality Gate.

Notes on ‚Äúindustry standard‚Äù realism

SonarQube Community is great for a lab, but some advanced features (e.g., branch analysis) are paid. Still absolutely resume-worthy.

This is exactly how many orgs do it: SonarQube as a shared platform service + Jenkins as orchestrator + quality gates.

Where this fits in your roadmap

Phase 4A: Trivy + Syft + Semgrep + SonarQube gate ‚úÖ

Then Phase 4C: Argo CD deploys only if CI passes gates ‚úÖ

Then Phase 4B: runtime governance (quotas, network policies, PSA) ‚úÖ

If you want, paste your current docker-compose.yml service list and your current Jenkinsfile path, and I‚Äôll give you a tight diff-style set of changes (compose + Jenkins config + pipeline stages) so you can implement SonarQube with minimal churn.

In the Dockerfile you gave, why not use jenkins/inbound-agent as the base image?

Great question ‚Äî and this is exactly the kind of architectural thinking that distinguishes platform engineering from just making things work.

Short answer:

‚ùå Do not use jenkins/inbound-agent as your tools image base.
‚úÖ Keep jnlp/inbound-agent separate from tooling containers.

Let‚Äôs unpack why, because this is an industry-standard pattern decision.

The Architectural Rule in Jenkins-on-Kubernetes

A Kubernetes Jenkins agent pod normally looks like:

Pod
 ‚îú‚îÄ‚îÄ jnlp container      ‚Üê Jenkins remoting runtime
 ‚îî‚îÄ‚îÄ tools container     ‚Üê build/scanning/tooling runtime


Two different responsibilities.

What jenkins/inbound-agent Is Designed For

The inbound-agent image exists only to:

Start Jenkins remoting

Authenticate to controller

Maintain agent connection

Handle workspace lifecycle

It is not meant to:

run build tooling

install scanners

contain language runtimes

include kubectl/docker/etc.

It is a protocol container, not a build environment.

Why You Should NOT Base Your Tools Image on inbound-agent
‚ùå 1. Bloated, unstable runtime

inbound-agent already contains:

Java runtime

Jenkins remoting

entrypoint scripts

agent boot logic

Adding tools on top creates:

Huge images

More attack surface

Slower pulls

Harder debugging

‚ùå 2. Breaks container responsibility model

If tools + agent live together:

jnlp container
   ‚îú‚îÄ‚îÄ remoting
   ‚îú‚îÄ‚îÄ trivy
   ‚îú‚îÄ‚îÄ semgrep
   ‚îú‚îÄ‚îÄ syft
   ‚îî‚îÄ‚îÄ kubectl


Now:

Agent crash = tool crash

Tool update risks agent

Harder isolation

‚ùå 3. Harder enterprise upgrades

In companies:

Jenkins plugin teams update agent versions

Platform teams update toolchains

They are independent lifecycles.

Combining them couples upgrades.

‚ùå 4. Security best practice violation

Security teams prefer:

Agent runtime     ‚Üí minimal, stable
Build runtime     ‚Üí disposable, isolated


Because scanners, compilers, package managers increase risk surface.

Industry Standard Layout

This is what large orgs use:

Pod
 ‚îú‚îÄ‚îÄ jnlp container (stable)
 ‚îú‚îÄ‚îÄ build container
 ‚îú‚îÄ‚îÄ scan container
 ‚îî‚îÄ‚îÄ optional sidecars


Example:

containers:
  - jnlp
  - docker-builder
  - security-tools
  - helm-tools


Each container serves a purpose.

When WOULD you base on inbound-agent?

Only in simpler setups:

Case 1 ‚Äî No Kubernetes

Classic static Jenkins agents:

VM agent
  ‚îú‚îÄ‚îÄ remoting
  ‚îî‚îÄ‚îÄ tools

Case 2 ‚Äî Tiny teams

One container per agent.

Not enterprise.

Why the snippet used Debian/Alpine instead

Because the tools container:

Doesn‚Äôt need Java

Doesn‚Äôt need remoting

Just needs CLI tools

Result:

Smaller image

Faster pulls

Cleaner separation

Easier upgrades

Professional Maturity Pattern

As organizations grow:

Stage 1:

jnlp + tools in same container


Stage 2:

jnlp + tools container


Stage 3:

jnlp + multiple specialized containers


Stage 4:

CI pipelines select container per stage


You're implementing Stage 2‚Äì3, which is already strong platform practice.

Resume-Level Reasoning

If an interviewer asks:

Why not use inbound-agent as the base image?

Strong answer:

Jenkins inbound-agent is dedicated to remoting and agent lifecycle. Separating it from build and security tooling allows independent lifecycle management, smaller images, reduced attack surface, and better stability. Enterprise Jenkins-on-Kubernetes environments typically keep agent runtime minimal and run build/scanning tools in separate containers.

That answer shows real experience.

Reality Check: Are you overengineering?

No. You're building a portfolio platform lab, not a demo.

This separation:

scales better

debugs easier

matches production CI platforms

Next Insight (optional preview)

Soon you'll discover:

tools container != builder container


Enterprises often use:

build containers

scan containers

deploy containers

per stage.

If you'd like, next we can build your final enterprise agent pod architecture so your lab resembles what Google/Netflix/Uber style CI platforms actually run.

This makes sense. Why use Debian over Ubuntu or other possible options? Was debian chosen intentionally?

Yes ‚Äî Debian was chosen intentionally, and the reasons tie directly to how production CI images are designed, not just personal preference.

Below is how platform engineers typically reason about base images.

The Real Question: What makes a good CI tools base image?

For CI tooling containers, we want:

Stability

Small attack surface

Good package availability

Predictable updates

Minimal surprises

Wide ecosystem compatibility

Debian hits a very strong balance across all of these.

Why Debian Slim is Common in CI/CD Platforms
1. Debian is upstream of Ubuntu

Ubuntu is based on Debian.

So:

Debian ‚Üí Ubuntu ‚Üí many enterprise distros


Using Debian avoids Ubuntu-specific modifications.

2. Debian Slim is smaller than Ubuntu

Compare approximate sizes:

Base image	Size
debian:bookworm-slim	~80 MB
ubuntu:22.04	~75‚Äì80 MB compressed but larger after installs
alpine	~7 MB
inbound-agent	300‚Äì400+ MB

Debian slim stays small while keeping compatibility.

3. Alpine is small but causes tool breakage

Many tutorials say:

Always use Alpine because it's small.

But in CI, Alpine causes pain.

Why?

Alpine uses:

musl libc


instead of:

glibc


Many precompiled binaries expect glibc.

Common breakages:

scanners

language toolchains

node packages

JVM tooling

security tools

terraform plugins

Result:

works locally ‚Üí fails in CI


Engineers waste hours debugging.

Debian avoids this.

4. Ubuntu adds unnecessary layers

Ubuntu images include:

extra services

system config

localization

additional tooling

Canonical patches

CI containers don't need that.

Ubuntu makes sense for:

developer VMs

desktops

long-running servers

Less sense for minimal CI images.

5. Enterprise vendors build against Debian/Ubuntu glibc

Many vendor binaries are tested against:

glibc + Debian/Ubuntu


So Debian gives best compatibility.

6. Security patching cadence

Debian stable releases:

predictable

conservative

well maintained

Good for CI reproducibility.

Why not use Alpine anyway?

Alpine is great for:

microservices

final runtime containers

small sidecars

Less great for:

CI build images

tool-heavy containers

scanners

language runtimes

Many companies try Alpine first, then migrate off it.

Why not use Ubuntu?

Ubuntu makes sense when:

engineers expect Ubuntu tooling

OS-level features matter

long-lived environments

But CI containers are disposable.

Debian slim gives:

smaller base

fewer surprises

fewer updates required

Real-world CI base images

Look at common enterprise bases:

Platform	Base
GitHub Actions runners	Ubuntu VM
GitLab runners	Debian/Ubuntu
Jenkins agents	Debian/Ubuntu
Google Cloud build	Debian
Many internal CI	Debian slim

Most large org CI images end up Debian-based.

Final reasoning used in your Dockerfile

I optimized for:

Small size
+ Compatibility
+ Stability
+ Minimal surprises


Debian slim hits the sweet spot.

Rule of Thumb for Platform Engineers
Use:

Debian slim ‚Üí CI tools containers

Alpine ‚Üí runtime containers

Distroless ‚Üí production app containers

Ubuntu ‚Üí VMs / developer environments

Advanced insight (future phase)

When your platform matures, you'll likely create:

ci-tools-base
   ‚îú‚îÄ‚îÄ language-specific variants
   ‚îú‚îÄ‚îÄ security-tools variants
   ‚îî‚îÄ‚îÄ deployment variants


Each still based on Debian slim.

If you‚Äôd like, next we can do a 5-minute deep dive into:

How companies structure internal CI images at scale

It will make Phase 4A decisions even clearer.

11.54 + pip3 install --no-cache-dir semgrep==1.103.0
11.93 error: externally-managed-environment
11.93 
11.93 √ó This environment is externally managed
11.93 ‚ï∞‚îÄ> To install Python packages system-wide, try apt install
11.93     python3-xyz, where xyz is the package you are trying to
11.93     install.
11.93     
11.93     If you wish to install a non-Debian-packaged Python package,
11.93     create a virtual environment using python3 -m venv path/to/venv.
11.93     Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
11.93     sure you have python3-full installed.
11.93     
11.93     If you wish to install a non-Debian packaged Python application,
11.93     it may be easiest to use pipx install xyz, which will manage a
11.93     virtual environment for you. Make sure you have pipx installed.
11.93     
11.93     See /usr/share/doc/python3.11/README.venv for more information.
11.93 
11.93 note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.
11.93 hint: See PEP 668 for the detailed specification.
------
Dockerfile:27
--------------------
  26 |     # Semgrep pinned (python)
  27 | >>> RUN apt-get update && apt-get install -y --no-install-recommends python3 python3-pip \
  28 | >>>     && pip3 install --no-cache-dir semgrep==1.103.0 \
  29 | >>>     && rm -rf /var/lib/apt/lists/*
  30 |     
--------------------
ERROR: failed to build: failed to solve: process "/bin/bash -euxo pipefail -c apt-get update && apt-get install -y --no-install-recommends python3 python3-pip     && pip3 install --no-cache-dir semgrep==1.103.0     && rm -rf /var/lib/apt/lists/*" did not complete successfully: exit code: 1

View build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/lkoh4pv6uetpqohz2wzfhxuec
ryanwaite28@Ryans-MacBook-Air ci-tools % 

Yep ‚Äî this is expected on modern Debian/Ubuntu now. You‚Äôve hit PEP 668 (‚Äúexternally managed environment‚Äù), which prevents pip from installing into the system Python site-packages inside distro-managed images.

You have 3 good, enterprise-grade fixes. I‚Äôll give you the recommended one for CI tools images, plus two alternatives.

‚úÖ Best practice fix: install Semgrep via pipx

pipx installs Python apps into an isolated venv and puts the binary on PATH ‚Äî perfect for CLI tools like Semgrep.

Update your Dockerfile Semgrep section to this:
# Semgrep pinned (pipx to avoid PEP 668)
ARG SEMGREP_VERSION=1.103.0
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip pipx \
 && pipx ensurepath \
 && pipx install "semgrep==${SEMGREP_VERSION}" \
 && ln -s /root/.local/bin/semgrep /usr/local/bin/semgrep \
 && rm -rf /var/lib/apt/lists/*

Why the symlink?

In Docker builds, pipx installs to /root/.local/bin by default. Symlinking to /usr/local/bin makes Semgrep available regardless of PATH quirks.

Verify after build:
docker run --rm ci-tools:0.1.0 semgrep --version

Alternative A: create a virtual environment and install Semgrep there

This is also clean and common.

ARG SEMGREP_VERSION=1.103.0
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-venv \
 && python3 -m venv /opt/venv \
 && /opt/venv/bin/pip install --no-cache-dir "semgrep==${SEMGREP_VERSION}" \
 && ln -s /opt/venv/bin/semgrep /usr/local/bin/semgrep \
 && rm -rf /var/lib/apt/lists/*


Pros: very explicit, deterministic
Cons: a little more boilerplate than pipx

Alternative B (works but not ideal): --break-system-packages
RUN pip3 install --no-cache-dir --break-system-packages semgrep==1.103.0


Pros: fastest
Cons: breaks the distro Python assumptions; not recommended for ‚Äúindustry standard‚Äù quality

My recommendation for your lab

Use pipx. It‚Äôs the cleanest way to ship Python CLIs in a tools image.

One more improvement (since you‚Äôre doing this ‚Äúenterprise‚Äù)

Pin pipx dependencies and keep the image lean:

Keep Semgrep in its own venv (pipx does this)

Don‚Äôt keep compilers/dev headers

Prefer Debian slim (you already are)

Next step after you patch the Dockerfile

Rebuild:

docker build -t ci-tools:0.1.0 pipelines/agent-images/ci-tools


Verify:

docker run --rm ci-tools:0.1.0 semgrep --version


Load into kind:

kind load docker-image ci-tools:0.1.0 --name kube-lab


If you paste your current Dockerfile (or just the section around Semgrep), I‚Äôll give you an exact drop-in replacement with line numbers so you can copy/paste cleanly.

ryanwaite28@Ryans-MacBook-Air ci-tools % docker run --rm ci-tools:0.1.0 semgrep --version
/root/.local/pipx/venvs/semgrep/bin/python: can't open file '/usr/local/bin/semgrep': [Errno 13] Permission denied
Thought for 10s

That error is a classic side-effect of the way we installed Semgrep with pipx:

pipx installed Semgrep under /root (/root/.local/pipx/...)

Your image later switches to a non-root user (USER ci) (or Docker may run non-root depending on your Dockerfile)

The semgrep ‚Äúshim‚Äù in /usr/local/bin/semgrep ends up pointing into root-owned paths and/or isn‚Äôt executable as the runtime user

The key clue is:

/root/.local/pipx/venvs/semgrep/bin/python: can't open file '/usr/local/bin/semgrep': Permission denied


So we need to install pipx apps into a shared, readable location, and expose the binary properly.

‚úÖ Fix: Use a global pipx home/bin (best practice)

Update your Dockerfile‚Äôs Semgrep section to this (drop-in replacement):

# Semgrep pinned (pipx in a global location to avoid PEP 668 + /root perms)
ARG SEMGREP_VERSION=1.103.0
ENV PIPX_HOME=/opt/pipx
ENV PIPX_BIN_DIR=/usr/local/bin

RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 python3-pip pipx \
 && pipx install "semgrep==${SEMGREP_VERSION}" \
 && chmod -R a+rX /opt/pipx \
 && chmod a+rx /usr/local/bin/semgrep \
 && rm -rf /var/lib/apt/lists/*

Why this works

PIPX_HOME=/opt/pipx keeps the venvs out of /root

PIPX_BIN_DIR=/usr/local/bin puts the executable where you expect it

chmod -R a+rX /opt/pipx ensures your later USER ci can read/execute it

üîÅ Rebuild + retest
docker build --no-cache -t ci-tools:0.1.0 pipelines/agent-images/ci-tools
docker run --rm ci-tools:0.1.0 semgrep --version


You should now get a normal Semgrep version output.

‚úÖ One extra check (important)

If your Dockerfile sets USER ci, verify Semgrep works as that user too:

docker run --rm --user 10001:10001 ci-tools:0.1.0 semgrep --version


(assuming you created ci with UID 10001)

After it works

Load into kind (so no Docker Hub pulls):

kind load docker-image ci-tools:0.1.0 --name kube-lab


If you paste your current ci-tools/Dockerfile (or at least the part where you switch USER), I‚Äôll point out any remaining permission gotchas (common ones are WORKDIR ownership and /workspace permissions).

ryanwaite28@Ryans-MacBook-Air ci-tools % docker run --rm ci-tools:0.1.0 sonar-scanner --version
rosetta error: failed to open elf at /lib64/ld-linux-x86-64.so.2
 %                                                                                                                                                      ryanwaite28@Ryans-MacBook-Air ci-tools %

You‚Äôve just hit an Apple Silicon architecture mismatch, not a SonarQube problem.

Your Mac is ARM64, but your Dockerfile downloaded the x86_64 sonar-scanner binary. Rosetta can‚Äôt handle this inside containers, hence:

rosetta error: failed to open elf at /lib64/ld-linux-x86-64.so.2


This is a very common CI-tools-image issue on M-series Macs.

Let‚Äôs fix this correctly and make your image multi-arch clean, which is what production platforms do.

Root cause

Your Dockerfile used:

sonar-scanner-*-linux-x64.zip


which only works on amd64.

But your environment is:

Mac M-series ‚Üí ARM64 ‚Üí kind nodes ARM64


So all binaries must match linux-arm64.

Correct fix: make downloads architecture-aware

Docker BuildKit automatically exposes the architecture via:

TARGETARCH


We‚Äôll map it correctly.

Fix sonar-scanner installation

Replace your sonar-scanner block with this:

ARG SONAR_SCANNER_VERSION=6.2.1.4610
ARG TARGETARCH

RUN set -eux; \
    if [ "$TARGETARCH" = "arm64" ]; then \
        SCANNER_ARCH="aarch64"; \
    else \
        SCANNER_ARCH="x64"; \
    fi; \
    curl -fsSL -o /tmp/sonar-scanner.zip \
      "https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-${SONAR_SCANNER_VERSION}-linux-${SCANNER_ARCH}.zip"; \
    apt-get update && apt-get install -y --no-install-recommends unzip; \
    unzip /tmp/sonar-scanner.zip -d /opt; \
    ln -s /opt/sonar-scanner-*/bin/sonar-scanner /usr/local/bin/sonar-scanner; \
    rm -f /tmp/sonar-scanner.zip; \
    rm -rf /var/lib/apt/lists/*

Then rebuild
docker build --no-cache -t ci-tools:0.1.0 .


Test:

docker run --rm ci-tools:0.1.0 sonar-scanner --version


Now it should succeed.

Why this matters beyond your laptop

This fix gives you:

‚úÖ Works on M1/M2/M3 Macs
‚úÖ Works on AMD64 CI servers
‚úÖ Works in kind/k3d clusters
‚úÖ Industry-standard multi-arch behavior

In real orgs:

dev laptops: arm64
CI runners: amd64
clusters: mixed


So multi-arch awareness is expected in platform images.

Bonus: you should update other tools too

If your Dockerfile currently downloads binaries assuming x64, update:

yq

trivy

syft

to use ${TARGETARCH} as well.

Quick confidence check

Run:

docker run --rm ci-tools:0.1.0 uname -m


Should output:

aarch64


meaning your container matches your cluster nodes.

Next step after this works

Once scanner + semgrep + trivy run correctly, next phase is:

Phase 4A.2 ‚Äî add scanning + quality gates to Jenkins pipeline

which will give you a fully enterprise CI pipeline.

If you‚Äôd like, paste your full Dockerfile and I‚Äôll harden it into a clean production-style CI tools image in one pass so you don‚Äôt keep hitting architecture issues.

ryanwaite28@Ryans-MacBook-Air ci-tools % docker run --rm ci-tools:0.1.0 sonar-scanner --version
23:48:52.832 INFO  Scanner configuration file: /opt/sonar-scanner-6.2.1.4610-linux-aarch64/conf/sonar-scanner.properties
23:48:52.838 INFO  Project root configuration file: NONE
23:48:52.862 INFO  SonarScanner CLI 6.2.1.4610
23:48:52.864 INFO  Java 17.0.12 Eclipse Adoptium (64-bit)
23:48:52.865 INFO  Linux 6.12.54-linuxkit aarch64
ryanwaite28@Ryans-MacBook-Air ci-tools % 
ryanwaite28@Ryans-MacBook-Air ci-tools % 
ryanwaite28@Ryans-MacBook-Air ci-tools % docker run --rm ci-tools:0.1.0 yq --version           
yq (https://github.com/mikefarah/yq/) version v4.44.3
ryanwaite28@Ryans-MacBook-Air ci-tools % docker run --rm ci-tools:0.1.0 syft version           
Application: syft
Version:    1.16.0
BuildDate:  2024-11-04T22:29:33Z
GitCommit:  8a41d772509d37267a65e0b425808e883e4b9dce
GitDescription: v1.16.0
Platform:   linux/arm64
GoVersion:  go1.22.8
Compiler:   gc
ryanwaite28@Ryans-MacBook-Air ci-tools % docker run --rm ci-tools:0.1.0 trivy --version        
Version: 0.56.2
ryanwaite28@Ryans-MacBook-Air ci-tools % 
ryanwaite28@Ryans-MacBook-Air ci-tools % docker run --rm ci-tools:0.1.0 semgrep --version      
/opt/pipx/venvs/semgrep/lib/python3.11/site-packages/opentelemetry/instrumentation/dependencies.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import (

A new version of Semgrep is available. See https://semgrep.dev/docs/upgrading
1.103.0
ryanwaite28@Ryans-MacBook-Air ci-tools % 

Nice ‚Äî that‚Äôs a clean, professional milestone. ‚úÖ
All critical tools run successfully on linux/arm64, and the versions are pinned. That‚Äôs exactly what we want for a resume-grade ‚Äúsecure CI tools image‚Äù.

What you have now

sonar-scanner ‚úÖ (aarch64 build working)

yq ‚úÖ

syft ‚úÖ

trivy ‚úÖ

semgrep ‚úÖ (the warning is harmless; more below)

About the Semgrep warning

This line:

pkg_resources is deprecated ... pin to Setuptools<81

It‚Äôs not breaking anything. It‚Äôs just a dependency warning from Python packaging.

If you want to keep your CI logs cleaner (optional ‚Äúpolish‚Äù), add a pin during the pipx install:

# after pipx install semgrep...
RUN /opt/pipx/venvs/semgrep/bin/pip install "setuptools<81"


But it‚Äôs totally fine to ignore for now.

‚úÖ Phase 4A.1: DONE

Your acceptance checks pass. Next we move to the real goal: