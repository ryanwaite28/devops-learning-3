âœ… Phase 3.3 is complete.
Your screenshot shows the gold-standard signals:

â€œAgent â€¦ is provisionedâ€

â€œRunning on â€¦â€

Stage executed

Finished: SUCCESS

kubectl get pods -w shows the full lifecycle: Pending â†’ Running â†’ Terminating

That is the core deliverable for Jenkins Cloud on Kubernetes.

About your â€œport 50000â€ discovery

Youâ€™re thinking in the right direction, but one important nuance:

In Kubernetes-agent mode, you do not need to expose port 50000

Port 50000 is for classic inbound JNLP agents (TCP).
Modern Kubernetes plugin setups typically use either:

WebSocket (preferred) over the Jenkins HTTP port (7000 for you), or

A remoting tunnel if configured

Your build succeeded with JENKINS_URL=http://host.docker.internal:7000/, which suggests the agent connected fine over HTTP/WebSocket or remoting handled internally.

Why your pod showed Error during termination

Thatâ€™s very commonly just the agent container exiting as Jenkins deletes the pod. When youâ€™re tailing -w, you can see transient Error as the container is killed during cleanupâ€”not necessarily a failure if the build is already SUCCESS.

âœ… Quick hardening to make this â€œenterprise-cleanâ€ (recommended)

Do these now to avoid confusing logs and reduce flakiness:

1) Enable WebSocket explicitly

In Manage Jenkins â†’ Clouds â†’ kubernetes-ci:

âœ… Check WebSocket

Save

This avoids dependence on port 50000 and is generally more reliable across NAT/Docker.

2) Stop using latest for the agent image

Pin to a specific inbound-agent version (any pinned tag is fine). This reduces pull failures and drift.

3) Add resource requests/limits to the jnlp container

Prevents the OOMKilled you saw earlier.

âœ… Phase 3.3 Acceptance Checklist (you meet it)

 Jenkins provisions ephemeral agent pods in ci

 Agent connects and executes pipeline steps

 Pod is terminated after completion (ephemeral behavior)

 Verified in both Jenkins console output and kubectl -w

Youâ€™re good.

ğŸš€ Next: Phase 3.4 â€” â€œEnterpriseâ€ Agent Pod Template (multi-container)
Current task

Create a standard reusable agent pod that has:

jnlp container (agent)

tools container (kubectl/helm/curl/git/trivy, etc.)

Explicit CPU/memory requests/limits

Runs as non-root (where feasible)

Why this matters

In real orgs, the JNLP container should stay â€œdumb.â€
Tooling belongs in sidecars so you can:

Keep remoting stable

Lock down permissions

Upgrade tools without breaking the agent runtime

Homework deliverable

Create a new pipeline that runs commands in the tools container:

pipeline {
  agent {
    kubernetes {
      label 'k8s-agent-tools'
      defaultContainer 'tools'
      yaml """
apiVersion: v1
kind: Pod
spec:
  serviceAccountName: jenkins-ci
  containers:
  - name: jnlp
    image: jenkins/inbound-agent:<PINNED>
    resources:
      requests: {cpu: "100m", memory: "256Mi"}
      limits: {cpu: "500m", memory: "512Mi"}
  - name: tools
    image: alpine:3.20
    command: ["sh", "-c", "cat"]
    tty: true
"""
    }
  }
  stages {
    stage('Tools container works') {
      steps {
        sh 'echo hello from tools && uname -a && cat /etc/os-release'
      }
    }
  }
}


When you finish, report back with:

Jenkins console output

kubectl get pods -n ci -w lifecycle

If you want the most resume-impressive route, after this weâ€™ll do:
Phase 4 â€” Secure supply chain: SBOM (Syft), image scan (Trivy), SAST (Semgrep), and policy gates.


-----

Me:
```
Explain that pipeline snippet you shared: the label has a new value "k8s-agent-tool" - do i create a new pod template in the kubernetes cloud? And the "yaml" part in pipeline.agent.kubernetes, does this have to match the containers defined in the pod template in the kubernetes cloud? Or is this another way of defining a pod template, in some ad-hoc fashion?
```

Great questions. This is the key concept in Jenkins-on-Kubernetes: where the pod template comes from, and how labels relate to templates.

1) What does the label 'k8s-agent-tools' mean?

That label is just a selector string Jenkins uses to match a dynamically-created agent to the pipeline that requested it.

Jenkins will create a pod for that agent.

When the pod connects back, Jenkins registers it as an agent node with that label.

The pipelineâ€™s agent { kubernetes { label '...' } } then runs on that newly registered agent.

So the label is basically: â€œuse the pod Jenkins creates for this pipeline.â€

Do you need to create a new Pod Template in Jenkins UI for the new label?

No, not if you define the pod in the Jenkinsfile via yaml.
You only need a UI pod template if you want a static, centrally-managed template (more on that below).

2) What is the yaml """ ... """ block?

This is Pipeline-as-Code pod template definition.

When you provide yaml under agent { kubernetes { ... } }, you are telling the Kubernetes plugin:

â€œFor this pipeline run, create the agent pod using this exact Kubernetes Pod spec.â€

That means it is ad-hoc per pipeline (but still versioned and repeatable because itâ€™s in Git).

Does it have to match anything in Jenkins UI?

No. If you provide yaml, Jenkins will use that YAML to create the pod, regardless of whatâ€™s in the UI.

3) So whatâ€™s the relationship between UI Pod Templates and Jenkinsfile yaml?

Think of it as two ways to supply the pod template, and you can mix them intentionally.

A) Inline template (Jenkinsfile yaml)

Defined in Git, per pipeline

Extremely portable/reproducible

Best for portfolios and â€œplatform as codeâ€

Great when teams want pipelines to define their runtime environment

âœ… This is what my snippet showed.

B) Static template (Jenkins UI â€œPod Templatesâ€)

Defined in Jenkins configuration

Reusable across many pipelines

Good for large orgs that want strict control

But less portable unless you also manage Jenkins config as code (JCasC)

âœ… Use this when you want standardization.

C) Hybrid: inheritFrom + overrides

This is the â€œenterprise sweet spot.â€

Define a base pod template in Jenkins UI (or JCasC)

In Jenkinsfile, reference it with inheritFrom

Optionally add/override YAML in the Jenkinsfile for small customizations

Example:
```groovy
pipeline {
  agent {
    kubernetes {
      inheritFrom 'base-ci-agent'
      label 'ci-run-123'
      yaml """ 
# optional additions/overrides 
"""
    }
  }
  stages { ... }
}
```

This is why Jenkins warns:

label is deprecatedâ€¦ use inheritFrom for static templates

Itâ€™s nudging you toward reusable templates for org-scale setups.

4) In your project, which approach should you use?

For portfolio + enterprise-quality without hidden UI config:

âœ… Recommended Path

Keep templates in the Jenkinsfile while youâ€™re building/learning

Once stable, migrate to Jenkins Configuration as Code (JCasC) so templates are still â€œas codeâ€ but centrally managed

Then use inheritFrom in pipelines

That shows you can do both:

Developer autonomy (pipeline-defined runtime)

Platform governance (standard templates)

5) Why did I use a new label like k8s-agent-tools?

Pure clarity.

It communicates:

this pipeline expects a pod with a â€œtoolsâ€ container

itâ€™s distinct from the minimal k8s-agent agent

You could reuse k8s-agent if you want, but I recommend distinct labels because it prevents confusion when you scale the lab.

Practical answer to your exact questions
Q: â€œDo I create a new pod template in Kubernetes cloud?â€

If you use Jenkinsfile yaml: No

If you want a reusable shared template in Jenkins UI: Yes

Q: â€œDoes the yaml have to match the UI template containers?â€

No, unless you are using inheritFrom (hybrid mode).

If youâ€™re not inheriting, the YAML is the template.

Q: â€œIs this another way of defining a pod template in ad-hoc fashion?â€

Yes â€” â€œad-hocâ€ but still versioned, repeatable, and auditable because it lives in Git.

Your next homework (to lock in understanding)

Do this in order:

Run a pipeline that uses only Jenkinsfile YAML (no UI templates).

Confirm the pod contains both containers (jnlp and tools):

kubectl get pod -n ci <pod> -o jsonpath='{.spec.containers[*].name}'


Then (optional) create a static template in UI called base-ci-agent, and update pipeline to:

inheritFrom 'base-ci-agent'


Explain in your README: when to choose each.

When you paste your current Jenkinsfile, Iâ€™ll refactor it into the cleanest â€œenterprise hybridâ€ pattern with inheritFrom + minimal overrides.

I think i'm done. Console/Terminal:

ryanwaite28@Ryans-MacBook-Air kube-platform-engineering-lab % kubectl get pods -n ci -w
NAME                          READY   STATUS    RESTARTS   AGE
k8s-agent-tools-zb55z-d76rm   0/2     Pending   0          0s
k8s-agent-tools-zb55z-d76rm   0/2     Pending   0          0s
k8s-agent-tools-zb55z-d76rm   0/2     ContainerCreating   0          0s
k8s-agent-tools-zb55z-d76rm   2/2     Running             0          3s
k8s-agent-tools-zb55z-d76rm   2/2     Terminating         0          41s
k8s-agent-tools-zb55z-d76rm   2/2     Terminating         0          42s
k8s-agent-tools-zb55z-d76rm   0/2     Error               0          73s
k8s-agent-tools-zb55z-d76rm   0/2     Error               0          73s
k8s-agent-tools-zb55z-d76rm   0/2     Error               0          73s
^C%                                                                                                                                                     ryanwaite28@Ryans-MacBook-Air kube-platform-engineering-lab % 

Jenkins:

Started by user Ryan Waite

[Pipeline] Start of Pipeline
[Pipeline] echo
[WARNING] label option is deprecated. To use a static pod template, use the 'inheritFrom' option.
[Pipeline] podTemplate
[Pipeline] {
[Pipeline] node
Still waiting to schedule task
â€˜k8s-agent-tools-9zrj9-0vf3x
â€™ is offline
Created Pod: kubernetes-ci ci/k8s-agent-tools-zb55z-d76rm
[PodInfo] ci/k8s-agent-tools-zb55z-d76rm
	Container [jnlp] waiting [ContainerCreating] No message
	Container [tools] waiting [ContainerCreating] No message
	Pod [Pending][ContainersNotReady] containers with unready status: [jnlp tools]

Agent k8s-agent-tools-zb55z-d76rm
 is provisioned from template k8s-agent-tools-zb55z
---
apiVersion: "v1"
kind: "Pod"
metadata:
  annotations:
    kubernetes.jenkins.io/last-refresh: "1769965043380"
    buildUrl: "http://host.docker.internal:7000/job/test-pipeline-2/3/"
    runUrl: "job/test-pipeline-2/3/"
  labels:
    jenkins: "slave"
    jenkins/label-digest: "6ce096d290e5eb0013b141b4bbf3d69e425d489a"
    jenkins/label: "k8s-agent-tools"
    kubernetes.jenkins.io/controller: "http___host_docker_internal_7000x"
  name: "k8s-agent-tools-zb55z-d76rm"
  namespace: "ci"
spec:
  containers:
  - env:
    - name: "JENKINS_SECRET"
      value: "********"
    - name: "JENKINS_AGENT_NAME"
      value: "k8s-agent-tools-zb55z-d76rm"
    - name: "JENKINS_WEB_SOCKET"
      value: "true"
    - name: "REMOTING_OPTS"
      value: "-noReconnectAfter 1d"
    - name: "JENKINS_NAME"
      value: "k8s-agent-tools-zb55z-d76rm"
    - name: "JENKINS_AGENT_WORKDIR"
      value: "/home/jenkins/agent"
    - name: "JENKINS_URL"
      value: "http://host.docker.internal:7000/"
    image: "jenkins/inbound-agent:jdk17"
    name: "jnlp"
    resources:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "256Mi"
    volumeMounts:
    - mountPath: "/home/jenkins/agent"
      name: "workspace-volume"
      readOnly: false
  - command:
    - "sh"
    - "-c"
    - "cat"
    image: "alpine:3.20"
    name: "tools"
    tty: true
    volumeMounts:
    - mountPath: "/home/jenkins/agent"
      name: "workspace-volume"
      readOnly: false
  nodeSelector:
    kubernetes.io/os: "linux"
  restartPolicy: "Never"
  serviceAccountName: "jenkins-ci"
  volumes:
  - emptyDir:
      medium: ""
    name: "workspace-volume"


Running on k8s-agent-tools-zb55z-d76rm
 in /home/jenkins/agent/workspace/test-pipeline-2
[Pipeline] {
[Pipeline] container
[Pipeline] {

[Pipeline] stage
[Pipeline] { (Tools container works)
[Pipeline] sh

+ echo hello from tools
hello from tools
+ uname -a
Linux k8s-agent-tools-zb55z-d76rm 6.12.54-linuxkit #1 SMP Fri Nov 21 10:33:45 UTC 2025 aarch64 Linux
+ cat /etc/os-release
NAME="Alpine Linux"
ID=alpine
VERSION_ID=3.20.9
PRETTY_NAME="Alpine Linux v3.20"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues"
[Pipeline] }
[Pipeline] // stage
[Pipeline] stage
[Pipeline] { (Declarative: Post Actions)
[Pipeline] emailext

Sending email to: dev-team@mailpit.local

[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // container
[Pipeline] }
[Pipeline] // node
[Pipeline] }
[Pipeline] // podTemplate

[Pipeline] End of Pipeline
Finished: SUCCESS

Youâ€™re done â€” and itâ€™s a clean pass. Phase 3.4 (multi-container agent pod) is COMPLETE âœ…

What proves it

kubectl -w shows a 2/2 pod lifecycle: Pending â†’ Running â†’ Terminating âœ…

Jenkins console shows:

Agent provisioned from template âœ…

serviceAccountName: jenkins-ci âœ…

JENKINS_WEB_SOCKET=true âœ…

You successfully executed commands inside the tools container âœ…

Email step fired (Mailpit integration) âœ…

Finished: SUCCESS âœ…

About the â€œagent is offlineâ€ line you saw

This is a common Jenkins Kubernetes plugin quirk: it may briefly log an â€œofflineâ€ agent while the pod is still coming up (ContainersNotReady / JNLP not connected yet). Since the agent did connect and the pipeline ran to success, that â€œofflineâ€ message was transient and harmless in this run.

If you want to reduce that noise:

Keep WebSocket enabled (you did)

Ensure the jnlp container has resources (you did)

Consider slightly increasing the Jenkins Cloud â€œSeconds to wait for pod to be runningâ€ (optional)

Why the pod shows Error at the end in kubectl -w

Also normal: when Jenkins deletes the pod, the container exits abruptly and Kubernetes may momentarily show Error during termination. The canonical truth is the pipeline result: SUCCESS.

If you want it prettier, set in Jenkins Cloud:

Pod Retention: Never (delete after run)

Or keep â€œOn failureâ€ (for debugging)

âœ… Phase Gate Summary (what you can now claim on your resume)

Youâ€™ve built:

Jenkins controller external to cluster (Docker)

Kubernetes as the execution substrate

Secure auth (TokenRequest + SA)

Least-privilege RBAC scoped to ci

Ephemeral agent pods with lifecycle cleanup

Multi-container agent design (jnlp + tools)

Resource limits to prevent OOM

Notification integration (Mailpit)

This is a legit CI platform.